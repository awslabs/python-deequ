{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing Computed Metrics in a MetricsRepository\n",
    "\n",
    "__Updated June 2024 to use with a new dataset and pydeequ 1.2.0/SPARK 3.3__\n",
    "\n",
    "PyDeequ allows us to persist the metrics we computed on dataframes in a so-called MetricsRepository. In the following example, we showcase how to store metrics in a filesystem and query them later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SPARK_VERSION\"] = '3.3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ec2-user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ec2-user/.ivy2/jars\n",
      "com.amazon.deequ#deequ added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-3fd29e82-4619-4f88-ba49-669eee4ba096;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.amazon.deequ#deequ;2.0.3-spark-3.3 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.10 in central\n",
      "\tfound org.scalanlp#breeze_2.12;0.13.2 in central\n",
      "\tfound org.scalanlp#breeze-macros_2.12;0.13.2 in central\n",
      "\tfound com.github.fommil.netlib#core;1.1.2 in central\n",
      "\tfound net.sf.opencsv#opencsv;2.3 in central\n",
      "\tfound com.github.rwl#jtransforms;2.4.0 in central\n",
      "\tfound junit#junit;4.8.2 in central\n",
      "\tfound org.apache.commons#commons-math3;3.2 in central\n",
      "\tfound org.spire-math#spire_2.12;0.13.0 in central\n",
      "\tfound org.spire-math#spire-macros_2.12;0.13.0 in central\n",
      "\tfound org.typelevel#machinist_2.12;0.6.1 in central\n",
      "\tfound com.chuusai#shapeless_2.12;2.3.2 in central\n",
      "\tfound org.typelevel#macro-compat_2.12;1.1.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.5 in central\n",
      ":: resolution report :: resolve 408ms :: artifacts dl 15ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazon.deequ#deequ;2.0.3-spark-3.3 from central in [default]\n",
      "\tcom.chuusai#shapeless_2.12;2.3.2 from central in [default]\n",
      "\tcom.github.fommil.netlib#core;1.1.2 from central in [default]\n",
      "\tcom.github.rwl#jtransforms;2.4.0 from central in [default]\n",
      "\tjunit#junit;4.8.2 from central in [default]\n",
      "\tnet.sf.opencsv#opencsv;2.3 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.2 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.10 from central in [default]\n",
      "\torg.scalanlp#breeze-macros_2.12;0.13.2 from central in [default]\n",
      "\torg.scalanlp#breeze_2.12;0.13.2 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.5 from central in [default]\n",
      "\torg.spire-math#spire-macros_2.12;0.13.0 from central in [default]\n",
      "\torg.spire-math#spire_2.12;0.13.0 from central in [default]\n",
      "\torg.typelevel#machinist_2.12;0.6.1 from central in [default]\n",
      "\torg.typelevel#macro-compat_2.12;1.1.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.scala-lang#scala-reflect;2.12.1 by [org.scala-lang#scala-reflect;2.12.10] in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.0 by [org.scala-lang#scala-reflect;2.12.10] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   17  |   0   |   0   |   2   ||   15  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-3fd29e82-4619-4f88-ba49-669eee4ba096\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 15 already retrieved (0kB/9ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/06/14 23:36:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/06/14 23:36:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/06/14 23:36:10 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/06/14 23:36:10 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row, DataFrame\n",
    "import json\n",
    "import pandas as pd\n",
    "import sagemaker_pyspark\n",
    "\n",
    "import pydeequ\n",
    "\n",
    "classpath = \":\".join(sagemaker_pyspark.classpath_jars())\n",
    "\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.driver.extraClassPath\", classpath)\n",
    "    .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord)\n",
    "    .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord)\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will be using the synthetic reviews dataset\n",
    "\n",
    "Specifically the Electronics and Books subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/06/14 23:36:38 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- marketplace: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_title: string (nullable = true)\n",
      " |-- star_rating: long (nullable = true)\n",
      " |-- helpful_votes: long (nullable = true)\n",
      " |-- total_votes: long (nullable = true)\n",
      " |-- insight: string (nullable = true)\n",
      " |-- review_headline: string (nullable = true)\n",
      " |-- review_body: string (nullable = true)\n",
      " |-- review_date: timestamp (nullable = true)\n",
      " |-- review_year: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- marketplace: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_title: string (nullable = true)\n",
      " |-- star_rating: long (nullable = true)\n",
      " |-- helpful_votes: long (nullable = true)\n",
      " |-- total_votes: long (nullable = true)\n",
      " |-- insight: string (nullable = true)\n",
      " |-- review_headline: string (nullable = true)\n",
      " |-- review_body: string (nullable = true)\n",
      " |-- review_date: timestamp (nullable = true)\n",
      " |-- review_year: long (nullable = true)\n",
      "\n",
      "None None\n"
     ]
    }
   ],
   "source": [
    "df_electronics = spark.read.parquet(\"s3a://aws-bigdata-blog/generated_synthetic_reviews/data/product_category=Electronics/\")\n",
    "\n",
    "df_books = spark.read.parquet(\"s3a://aws-bigdata-blog/generated_synthetic_reviews/data/product_category=Books/\")\n",
    "\n",
    "print(df_electronics.printSchema(), df_books.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Metrics Repository\n",
    "\n",
    "We will be demoing with the `FileSystemMetricsRepository` class, but you can optionally use `InMemoryMetricsRepository` the exact same way without creating a `metrics_file` like so: `repository = InMemoryMetricsRepository(spark)`. \n",
    "\n",
    "**Metrics Repository allows us to store the metrics in json format on the local disk (note that it also supports HDFS and S3).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics_file path: /tmp/1718408214845-0/metrics.json\n"
     ]
    }
   ],
   "source": [
    "from pydeequ.repository import *\n",
    "\n",
    "metrics_file = FileSystemMetricsRepository.helper_metrics_file(spark, 'metrics.json')\n",
    "print(f'metrics_file path: {metrics_file}')\n",
    "repository = FileSystemMetricsRepository(spark, metrics_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Each set of metrics that we computed needs be indexed by a so-called `ResultKey`, which contains a timestamp and supports arbitrary tags in the form of key-value pairs. Let's setup one for this example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "key_tags = {'tag': 'electronics'}\n",
    "resultKey = ResultKey(spark, ResultKey.current_milli_time(), key_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will be building off the Analyzers basic tutorial ... including Metrics Repository into it! \n",
    "\n",
    "Now we can run checks or analyzers on our data as usual. However, we make deequ store the resulting metrics for the checks in our repository by adding the `useRepository` and `saveOrAppendResult` methods to our invocation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/06/14 23:37:18 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-------------------+--------------------+\n",
      "|     entity|            instance|               name|               value|\n",
      "+-----------+--------------------+-------------------+--------------------+\n",
      "|     Column|           review_id|       Completeness|                 1.0|\n",
      "|     Column|           review_id|ApproxCountDistinct|           3160409.0|\n",
      "|Mutlicolumn|total_votes,star_...|        Correlation|-7.38808965018615...|\n",
      "|    Dataset|                   *|               Size|           3010972.0|\n",
      "|     Column|         star_rating|               Mean|  3.9999973430506826|\n",
      "|     Column|     top star_rating|         Compliance|  0.7499993357626706|\n",
      "|Mutlicolumn|total_votes,helpf...|        Correlation|  0.9817922803462663|\n",
      "+-----------+--------------------+-------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    }
   ],
   "source": [
    "from pydeequ.analyzers import *\n",
    "\n",
    "analysisResult = AnalysisRunner(spark) \\\n",
    "                    .onData(df_electronics) \\\n",
    "                    .addAnalyzer(Size()) \\\n",
    "                    .addAnalyzer(Completeness(\"review_id\")) \\\n",
    "                    .addAnalyzer(ApproxCountDistinct(\"review_id\")) \\\n",
    "                    .addAnalyzer(Mean(\"star_rating\")) \\\n",
    "                    .addAnalyzer(Compliance(\"top star_rating\", \"star_rating >= 4.0\")) \\\n",
    "                    .addAnalyzer(Correlation(\"total_votes\", \"star_rating\")) \\\n",
    "                    .addAnalyzer(Correlation(\"total_votes\", \"helpful_votes\")) \\\n",
    "                    .useRepository(repository) \\\n",
    "                    .saveOrAppendResult(resultKey) \\\n",
    "                    .run()\n",
    "                    \n",
    "analysisResult_df = AnalyzerContext.successMetricsAsDataFrame(spark, analysisResult)\n",
    "analysisResult_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can load it back now from Metrics Repository \n",
    "\n",
    "PyDeequ now executes the verification as usual and additionally stores the metrics under our specified key. Afterwards, we can retrieve the metrics from the repository in different ways. We can for example directly load the metric for a particular analyzer stored under our result key as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-------------------+--------------------+-------------+-----------+\n",
      "|     entity|            instance|               name|               value| dataset_date|        tag|\n",
      "+-----------+--------------------+-------------------+--------------------+-------------+-----------+\n",
      "|     Column|           review_id|       Completeness|                 1.0|1718408220742|electronics|\n",
      "|     Column|           review_id|ApproxCountDistinct|           3160409.0|1718408220742|electronics|\n",
      "|Mutlicolumn|total_votes,star_...|        Correlation|-7.38808965018615...|1718408220742|electronics|\n",
      "|    Dataset|                   *|               Size|           3010972.0|1718408220742|electronics|\n",
      "|     Column|         star_rating|               Mean|  3.9999973430506826|1718408220742|electronics|\n",
      "|     Column|     top star_rating|         Compliance|  0.7499993357626706|1718408220742|electronics|\n",
      "|Mutlicolumn|total_votes,helpf...|        Correlation|  0.9817922803462663|1718408220742|electronics|\n",
      "+-----------+--------------------+-------------------+--------------------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analysisResult_metRep = repository.load() \\\n",
    "                            .before(ResultKey.current_milli_time()) \\\n",
    "                            .getSuccessMetricsAsDataFrame()\n",
    "\n",
    "analysisResult_metRep.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But that's not very interesting... Let's run another Analysis on the books dataset! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=======================================================> (33 + 1) / 34]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-------------------+--------------------+\n",
      "|     entity|            instance|               name|               value|\n",
      "+-----------+--------------------+-------------------+--------------------+\n",
      "|     Column|           review_id|       Completeness|                 1.0|\n",
      "|     Column|           review_id|ApproxCountDistinct|         1.0865041E7|\n",
      "|Mutlicolumn|total_votes,star_...|        Correlation|1.747345622996871...|\n",
      "|    Dataset|                   *|               Size|           9672664.0|\n",
      "|     Column|         star_rating|               Mean|  2.9938504015026264|\n",
      "|     Column|     top star_rating|         Compliance| 0.33738967878962817|\n",
      "|Mutlicolumn|total_votes,helpf...|        Correlation|8.085328839629536E-5|\n",
      "+-----------+--------------------+-------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "key_tags_2 = {'tag': 'books'}\n",
    "resultKey_2 = ResultKey(spark, ResultKey.current_milli_time(), key_tags_2)\n",
    "\n",
    "analysisResult_2 = AnalysisRunner(spark) \\\n",
    "                    .onData(df_books) \\\n",
    "                    .addAnalyzer(Size()) \\\n",
    "                    .addAnalyzer(Completeness(\"review_id\")) \\\n",
    "                    .addAnalyzer(ApproxCountDistinct(\"review_id\")) \\\n",
    "                    .addAnalyzer(Mean(\"star_rating\")) \\\n",
    "                    .addAnalyzer(Compliance(\"top star_rating\", \"star_rating >= 4.0\")) \\\n",
    "                    .addAnalyzer(Correlation(\"total_votes\", \"star_rating\")) \\\n",
    "                    .addAnalyzer(Correlation(\"total_votes\", \"helpful_votes\")) \\\n",
    "                    .useRepository(repository) \\\n",
    "                    .saveOrAppendResult(resultKey_2) \\\n",
    "                    .run()\n",
    "\n",
    "analysisResult_2_df = AnalyzerContext.successMetricsAsDataFrame(spark, analysisResult_2)\n",
    "analysisResult_2_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we should see two different tags when we load it back from Metrics Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------------+-------------------+---------------------+-------------+-----------+\n",
      "|entity     |instance                 |name               |value                |dataset_date |tag        |\n",
      "+-----------+-------------------------+-------------------+---------------------+-------------+-----------+\n",
      "|Column     |review_id                |Completeness       |1.0                  |1718408220742|electronics|\n",
      "|Column     |review_id                |ApproxCountDistinct|3160409.0            |1718408220742|electronics|\n",
      "|Mutlicolumn|total_votes,star_rating  |Correlation        |-7.388089650186156E-4|1718408220742|electronics|\n",
      "|Dataset    |*                        |Size               |3010972.0            |1718408220742|electronics|\n",
      "|Column     |star_rating              |Mean               |3.9999973430506826   |1718408220742|electronics|\n",
      "|Column     |top star_rating          |Compliance         |0.7499993357626706   |1718408220742|electronics|\n",
      "|Mutlicolumn|total_votes,helpful_votes|Correlation        |0.9817922803462663   |1718408220742|electronics|\n",
      "|Column     |review_id                |Completeness       |1.0                  |1718408257243|books      |\n",
      "|Column     |review_id                |ApproxCountDistinct|1.0865041E7          |1718408257243|books      |\n",
      "|Mutlicolumn|total_votes,star_rating  |Correlation        |1.7473456229968713E-4|1718408257243|books      |\n",
      "|Dataset    |*                        |Size               |9672664.0            |1718408257243|books      |\n",
      "|Column     |star_rating              |Mean               |2.9938504015026264   |1718408257243|books      |\n",
      "|Column     |top star_rating          |Compliance         |0.33738967878962817  |1718408257243|books      |\n",
      "|Mutlicolumn|total_votes,helpful_votes|Correlation        |8.085328839629536E-5 |1718408257243|books      |\n",
      "+-----------+-------------------------+-------------------+---------------------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analysisResult_metRep_2 = repository.load() \\\n",
    "                            .before(ResultKey.current_milli_time()) \\\n",
    "                            .getSuccessMetricsAsDataFrame()\n",
    "\n",
    "analysisResult_metRep_2.show(analysisResult_metRep_2.count(), False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see the differences in the `dataset_date` and `tag` column and filter our results like so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------------+-------------------+---------------------+-------------+-----+\n",
      "|entity     |instance                 |name               |value                |dataset_date |tag  |\n",
      "+-----------+-------------------------+-------------------+---------------------+-------------+-----+\n",
      "|Column     |review_id                |Completeness       |1.0                  |1718408257243|books|\n",
      "|Column     |review_id                |ApproxCountDistinct|1.0865041E7          |1718408257243|books|\n",
      "|Mutlicolumn|total_votes,star_rating  |Correlation        |1.7473456229968713E-4|1718408257243|books|\n",
      "|Dataset    |*                        |Size               |9672664.0            |1718408257243|books|\n",
      "|Column     |star_rating              |Mean               |2.9938504015026264   |1718408257243|books|\n",
      "|Column     |top star_rating          |Compliance         |0.33738967878962817  |1718408257243|books|\n",
      "|Mutlicolumn|total_votes,helpful_votes|Correlation        |8.085328839629536E-5 |1718408257243|books|\n",
      "+-----------+-------------------------+-------------------+---------------------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_tags = repository.load() \\\n",
    "        .withTagValues(key_tags_2) \\\n",
    "        .getSuccessMetricsAsDataFrame()\n",
    "\n",
    "filtered_tags.show(filtered_tags.count(), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------------+-------------------+---------------------+-------------+-----------+\n",
      "|entity     |instance                 |name               |value                |dataset_date |tag        |\n",
      "+-----------+-------------------------+-------------------+---------------------+-------------+-----------+\n",
      "|Column     |review_id                |Completeness       |1.0                  |1718408220742|electronics|\n",
      "|Column     |review_id                |ApproxCountDistinct|3160409.0            |1718408220742|electronics|\n",
      "|Mutlicolumn|total_votes,star_rating  |Correlation        |-7.388089650186156E-4|1718408220742|electronics|\n",
      "|Dataset    |*                        |Size               |3010972.0            |1718408220742|electronics|\n",
      "|Column     |star_rating              |Mean               |3.9999973430506826   |1718408220742|electronics|\n",
      "|Column     |top star_rating          |Compliance         |0.7499993357626706   |1718408220742|electronics|\n",
      "|Mutlicolumn|total_votes,helpful_votes|Correlation        |0.9817922803462663   |1718408220742|electronics|\n",
      "|Column     |review_id                |Completeness       |1.0                  |1718408257243|books      |\n",
      "|Column     |review_id                |ApproxCountDistinct|1.0865041E7          |1718408257243|books      |\n",
      "|Mutlicolumn|total_votes,star_rating  |Correlation        |1.7473456229968713E-4|1718408257243|books      |\n",
      "|Dataset    |*                        |Size               |9672664.0            |1718408257243|books      |\n",
      "|Column     |star_rating              |Mean               |2.9938504015026264   |1718408257243|books      |\n",
      "|Column     |top star_rating          |Compliance         |0.33738967878962817  |1718408257243|books      |\n",
      "|Mutlicolumn|total_votes,helpful_votes|Correlation        |8.085328839629536E-5 |1718408257243|books      |\n",
      "+-----------+-------------------------+-------------------+---------------------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_time = repository.load() \\\n",
    "        .after(1595457441235+1) \\\n",
    "        .getSuccessMetricsAsDataFrame()\n",
    "\n",
    "filtered_time.show(filtered_time.count(), False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For more info ... look at full list of Metrics Repository in `docs/repository.md` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
